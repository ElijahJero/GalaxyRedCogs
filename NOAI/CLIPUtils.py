import io
import math
import asyncio
from dataclasses import dataclass
from typing import Optional, Dict, List, Tuple, Any

from PIL import Image

import torch
from transformers import CLIPProcessor, CLIPModel

_CLIP_MODEL: Optional[CLIPModel] = None
_CLIP_PROCESSOR: Optional[CLIPProcessor] = None
_CLIP_DEVICE: str = "cpu"


@dataclass
class SmartAnalysisReport:
    ok: bool
    error: Optional[str]

    image_type: str
    image_type_confidence: float
    type_probs: Dict[str, float]

    ai_likelihood: float  # always 0..100
    certainty: str
    verdict: str
    warning: Optional[str]

    details: Dict[str, Any]


def _sigmoid(x: float) -> float:
    if x >= 0:
        z = math.exp(-x)
        return 1.0 / (1.0 + z)
    else:
        z = math.exp(x)
        return z / (1.0 + z)


def _load_clip_model():
    global _CLIP_MODEL, _CLIP_PROCESSOR, _CLIP_DEVICE
    if _CLIP_MODEL is not None and _CLIP_PROCESSOR is not None:
        return

    _CLIP_DEVICE = "cpu"
    _CLIP_MODEL = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(_CLIP_DEVICE)
    _CLIP_MODEL.eval()
    _CLIP_PROCESSOR = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")


def _open_image(image_bytes: bytes) -> Image.Image:
    try:
        return Image.open(io.BytesIO(image_bytes)).convert("RGB")
    except Exception as e:
        raise RuntimeError(f"Failed to open image: {e}")


def _resize_image_for_cpu(img: Image.Image, max_side: int = 768) -> Image.Image:
    w, h = img.size
    m = max(w, h)
    if m <= max_side:
        return img
    scale = max_side / float(m)
    new_w = max(1, int(round(w * scale)))
    new_h = max(1, int(round(h * scale)))
    return img.resize((new_w, new_h), resample=Image.BICUBIC)


def _clip_similarities(img: Image.Image, prompts: List[str]) -> torch.Tensor:
    global _CLIP_MODEL, _CLIP_PROCESSOR, _CLIP_DEVICE
    if _CLIP_MODEL is None or _CLIP_PROCESSOR is None:
        _load_clip_model()

    inputs = _CLIP_PROCESSOR(text=prompts, images=img, return_tensors="pt", padding=True)
    inputs = {k: v.to(_CLIP_DEVICE) for k, v in inputs.items()}
    with torch.no_grad():
        outputs = _CLIP_MODEL(**inputs)
    return outputs.logits_per_image[0].detach().cpu()


# Router prompt groups
_TYPE_PROMPTS: Dict[str, List[str]] = {
    "photo": [
        "a real photograph",
        "a phone photo",
        "a DSLR photo",
        "a natural unedited photograph",
    ],
    "illustration": [
        "an anime illustration",
        "a cartoon drawing",
        "a digital painting",
        "concept art",
    ],
    "screenshot": [
        "a screenshot",
        "a meme with text",
        "a screenshot of a website",
    ],
    "render": [
        "a 3d render",
        "CGI",
        "computer generated imagery",
    ],
}

# Hybrid scoring prompt packs:
# - For photo: try AI-photo vs real-photo
# - For non-photo: revert closer to your original "AI-ish / digital artwork" signal
_PROMPTS_BY_TYPE: Dict[str, Dict[str, List[str]]] = {
    "photo": {
        "ai": [
            "an AI generated image",
            "a photo generated by AI",
            "a synthetic photograph",
        ],
        "real": [
            "a real photograph taken by a human",
            "a natural unedited photo",
            "a candid camera photo",
        ],
    },
    # Non-photo types intentionally use "digitally generated artwork" style prompts
    # because CLIP cannot reliably do AI-art vs human-art separation.
    "illustration": {
        "ai": [
            "an AI generated image",
            "a digitally generated artwork",
            "AI generated art",
        ],
        "real": [
            "a real photograph taken by a human",
            "a natural unedited photo",
        ],
    },
    "render": {
        "ai": [
            "an AI generated image",
            "a digitally generated artwork",
            "a computer generated image",
        ],
        "real": [
            "a real photograph taken by a human",
            "a natural unedited photo",
        ],
    },
    "screenshot": {
        "ai": [
            "an AI generated image",
            "a digitally generated artwork",
        ],
        "real": [
            "a screenshot",
            "a normal screenshot of an app",
        ],
    },
}


def _sync_route_image_type(img: Image.Image) -> Tuple[str, float, Dict[str, float]]:
    group_names: List[str] = []
    flat_prompts: List[str] = []
    for g, ps in _TYPE_PROMPTS.items():
        for p in ps:
            group_names.append(g)
            flat_prompts.append(p)

    sims = _clip_similarities(img, flat_prompts)

    # mean per group
    group_sums: Dict[str, float] = {k: 0.0 for k in _TYPE_PROMPTS.keys()}
    group_counts: Dict[str, int] = {k: 0 for k in _TYPE_PROMPTS.keys()}
    for s, g in zip(sims.tolist(), group_names):
        group_sums[g] += float(s)
        group_counts[g] += 1

    group_means = {g: group_sums[g] / max(1, group_counts[g]) for g in group_sums.keys()}

    keys = list(group_means.keys())
    vals = torch.tensor([group_means[k] for k in keys], dtype=torch.float32)
    probs_t = torch.softmax(vals, dim=0)
    probs = {k: float(p) for k, p in zip(keys, probs_t.tolist())}

    best_type = max(probs, key=probs.get)
    confidence = probs[best_type]
    return best_type, confidence, probs


def _sync_score(img: Image.Image, image_type: str) -> Tuple[float, Dict[str, Any]]:
    """
    Returns a conservative AI-likelihood estimate based on *relative* CLIP signal strength.
    This is a weak stylistic signal, NOT provenance.
    """

    pack = _PROMPTS_BY_TYPE.get(image_type) or _PROMPTS_BY_TYPE["photo"]
    ai_prompts = pack["ai"]
    real_prompts = pack["real"]

    ai_sims = _clip_similarities(img, ai_prompts)
    real_sims = _clip_similarities(img, real_prompts)

    sim_ai = ai_sims.mean().item()
    sim_real = real_sims.mean().item()
    margin = sim_ai - sim_real

    # --- NORMALIZATION ---
    # CLIP margins are meaningless in absolute terms.
    # We normalize by the combined standard deviation.
    combined = torch.cat([ai_sims, real_sims])
    std = combined.std().item()

    if std < 1e-6:
        normalized = 0.0
    else:
        normalized = margin / std

    # --- INTERPRETATION ---
    # These thresholds are intentionally conservative.
    if normalized >= 1.2:
        ai_likelihood = 65.0
    elif normalized >= 0.7:
        ai_likelihood = 55.0
    elif normalized <= -1.2:
        ai_likelihood = 35.0
    elif normalized <= -0.7:
        ai_likelihood = 45.0
    else:
        ai_likelihood = 50.0

    # Hard cap â€” CLIP alone should never be decisive
    ai_likelihood = max(30.0, min(70.0, ai_likelihood))

    top_ai_i = int(torch.argmax(ai_sims).item())
    top_real_i = int(torch.argmax(real_sims).item())

    return ai_likelihood, {
        "sim_ai_mean": round(sim_ai, 4),
        "sim_real_mean": round(sim_real, 4),
        "margin": round(margin, 4),
        "normalized_margin": round(normalized, 3),
        "std": round(std, 4),
        "top_ai_prompt": {
            "prompt": ai_prompts[top_ai_i],
            "similarity": round(ai_sims[top_ai_i].item(), 4),
        },
        "top_real_prompt": {
            "prompt": real_prompts[top_real_i],
            "similarity": round(real_sims[top_real_i].item(), 4),
        },
        "note": "CLIP provides a weak stylistic signal, not proof of AI generation.",
    }



def _warning_for_type(image_type: str) -> Optional[str]:
    if image_type == "photo":
        return None
    if image_type == "illustration":
        return "Warning: For illustrations/anime, this score is often inaccurate (CLIP is not an AI-art detector)."
    if image_type == "render":
        return "Warning: For CGI/3D renders, this score is often inaccurate (CLIP is not a provenance detector)."
    if image_type == "screenshot":
        return "Warning: For screenshots/memes, this score is very unreliable."
    return "Warning: This score may be inaccurate for this image type."


def _certainty(image_type: str, type_conf: float, normalized_margin: float) -> str:
    if image_type != "photo":
        return "Low"

    if type_conf < 0.9:
        return "Low"

    if abs(normalized_margin) >= 1.2:
        return "Medium"

    return "Low"

def _verdict(ai_likelihood: float, certainty: str, image_type: str) -> str:
    if certainty == "Low":
        return "Uncertain"

    if ai_likelihood >= 60:
        return "May resemble AI-generated imagery"
    if ai_likelihood <= 40:
        return "May resemble human photography"

    return "Uncertain"



def _sync_analyze_image_smart(image_bytes: bytes) -> SmartAnalysisReport:
    img = None
    try:
        img = _open_image(image_bytes)
        img = _resize_image_for_cpu(img, max_side=768)

        image_type, type_conf, type_probs = _sync_route_image_type(img)
        ai_likelihood, score_details = _sync_score(img, image_type)
        margin = float(score_details.get("margin", 0.0))

        certainty = _certainty(image_type, type_conf, margin)
        verdict = _verdict(ai_likelihood, certainty, image_type)
        warning = _warning_for_type(image_type)

        return SmartAnalysisReport(
            ok=True,
            error=None,
            image_type=image_type,
            image_type_confidence=round(type_conf, 3),
            type_probs={k: round(v, 3) for k, v in type_probs.items()},
            ai_likelihood=ai_likelihood,
            certainty=certainty,
            verdict=verdict,
            warning=warning,
            details={"scoring": score_details},
        )
    except Exception as e:
        return SmartAnalysisReport(
            ok=False,
            error=str(e),
            image_type="unknown",
            image_type_confidence=0.0,
            type_probs={},
            ai_likelihood=0.0,
            certainty="Low",
            verdict="Error",
            warning="Error during analysis.",
            details={},
        )
    finally:
        if img is not None:
            try:
                img.close()
            except Exception:
                pass


async def analyze_image_smart(self, data: bytes, filename: str, url: str, ctx) -> SmartAnalysisReport:
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(None, _sync_analyze_image_smart, data)


# Legacy API
async def analize_image(self, data: bytes, filename: str, url: str, ctx) -> float:
    report = await analyze_image_smart(self, data, filename, url, ctx)
    return -1 if not report.ok else float(report.ai_likelihood)


def certainty_string_generator(certainty_score) -> str:
    # Kept for backwards compatibility
    try:
        v = float(certainty_score)
    except Exception:
        return "Uncertain"
    if v > 80:
        return "Very likely AI generated"
    elif v > 60:
        return "Probably AI generated"
    elif v > 40:
        return "Uncertain if AI generated"
    elif v > 20:
        return "Probably human made or modified"
    else:
        return "Likely human made or modified"